# üîå Module 06 - Kafka Connect : Int√©gration de Donn√©es

| Dur√©e | Niveau | Pr√©requis |
|-------|--------|-----------|
| 2 heures | Interm√©diaire | Modules 01-05 compl√©t√©s |

## üéØ Objectifs d'apprentissage

√Ä la fin de ce module, vous serez capable de :

- ‚úÖ Comprendre l'architecture de Kafka Connect
- ‚úÖ D√©ployer un connecteur Source (fichier ‚Üí Kafka)
- ‚úÖ D√©ployer un connecteur Sink (Kafka ‚Üí fichier)
- ‚úÖ Configurer et monitorer les connecteurs

---

## üìö Partie Th√©orique (30%)

### 1. Introduction √† Kafka Connect

#### Qu'est-ce que Kafka Connect ?

**Kafka Connect** est un framework d'int√©gration de donn√©es scalable et fiable pour connecter Kafka √† des syst√®mes externes (bases de donn√©es, fichiers, APIs, etc.).

```mermaid
flowchart LR
    SRC["üóÑÔ∏è Sources"] --> SC["üîå Source"] --> T[("ÔøΩ Kafka")] --> SK["ÔøΩ Sink"] --> SNK["üóÑÔ∏è Sinks"]
    
    style SC fill:#e8f5e9
    style SK fill:#fff3cd
```

#### Concepts cl√©s

| Concept | Description |
|---------|-------------|
| **Connector** | Plugin qui d√©finit comment se connecter √† un syst√®me externe |
| **Task** | Unit√© de travail parall√©lisable du connecteur |
| **Worker** | Processus JVM qui ex√©cute les connecteurs et tasks |
| **Converter** | Transforme les donn√©es entre Kafka et le format du connecteur |

---

### 2. Types de connecteurs

```mermaid
flowchart LR
    subgraph src["üì• SOURCE"]
        E1["DB/File"] -->|read| S1["üîå"] -->|produce| K1["Kafka"]
    end
    subgraph snk["üì§ SINK"]
        K2["Kafka"] -->|consume| S2["üîå"] -->|write| E2["DB/S3"]
    end
    style src fill:#e8f5e9
    style snk fill:#fff3cd
```

#### Connecteurs populaires

| Type | Connecteur | Usage |
|------|------------|-------|
| Source | JDBC Source | Importer depuis SQL |
| Source | Debezium | CDC (Change Data Capture) |
| Source | FileStream | Importer depuis fichiers |
| Sink | JDBC Sink | Exporter vers SQL |
| Sink | Elasticsearch | Indexation |
| Sink | S3 Sink | Archivage cloud |

---

### 3. Modes de d√©ploiement

```mermaid
flowchart LR
    subgraph sa["üíª Standalone"]
        W1["1 Worker"]
    end
    subgraph di["‚òÅÔ∏è Distributed"]
        W2["W1"]
        W3["W2"]
    end
    style di fill:#e8f5e9
```

| Mode | Avantages | Inconv√©nients |
|------|-----------|---------------|
| **Standalone** | Simple, Dev/Test | Non HA, Single machine |
| **Distributed** | Scalable, Fault-tolerant | Plus complexe |

---

### 4. Configuration d'un connecteur

```json
{
  "name": "file-source-connector",
  "config": {
    "connector.class": "FileStreamSource",
    "tasks.max": "1",
    "file": "/data/input.txt",
    "topic": "file-topic",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.storage.StringConverter"
  }
}
```

#### Param√®tres essentiels

| Param√®tre | Description |
|-----------|-------------|
| `connector.class` | Classe Java du connecteur |
| `tasks.max` | Nombre max de tasks parall√®les |
| `key.converter` | Convertisseur pour les cl√©s |
| `value.converter` | Convertisseur pour les valeurs |

---

### 5. Change Data Capture (CDC) avec Debezium

Le **CDC** permet de capturer les changements de donn√©es en temps r√©el depuis une base de donn√©es vers Kafka.

```mermaid
flowchart LR
    subgraph DB["üóÑÔ∏è Base de donn√©es"]
        T["Table Orders"]
        WAL["üìù WAL/Transaction Log"]
    end
    
    subgraph Debezium["üîå Debezium"]
        CDC["CDC Connector"]
    end
    
    subgraph Kafka["üì¶ Kafka"]
        TP["orders.public.orders"]
    end
    
    T -->|"INSERT/UPDATE/DELETE"| WAL
    WAL -->|"Stream changes"| CDC
    CDC -->|"Produce"| TP
    
    style WAL fill:#fff3cd
    style CDC fill:#e8f5e9
```

#### Pourquoi CDC vs Polling ?

| Approche | Avantages | Inconv√©nients |
|----------|-----------|---------------|
| **Polling (JDBC)** | Simple √† configurer | Latence, charge DB, DELETE non captur√©s |
| **CDC (Debezium)** | Temps r√©el, tous les changements, faible impact | Configuration WAL requise |

#### Structure d'un √©v√©nement CDC

```json
{
  "before": { "id": 1, "status": "pending" },
  "after": { "id": 1, "status": "shipped" },
  "source": {
    "db": "orders_db",
    "table": "orders",
    "ts_ms": 1706450400000
  },
  "op": "u"
}
```

| Champ | Description |
|-------|-------------|
| `before` | √âtat avant modification (null pour INSERT) |
| `after` | √âtat apr√®s modification (null pour DELETE) |
| `op` | Op√©ration: `c`=create, `u`=update, `d`=delete, `r`=read |

---

### 6. CDC avec PostgreSQL

PostgreSQL utilise le **WAL (Write-Ahead Log)** avec le plugin `pgoutput` pour le CDC.

```mermaid
flowchart TB
    subgraph PostgreSQL
        APP["üñ•Ô∏è Application .NET"] -->|"Entity Framework"| PG["üêò PostgreSQL"]
        PG -->|"pgoutput"| WAL["üìù WAL"]
    end
    
    WAL -->|"Logical Replication"| DEB["üîå Debezium"]
    DEB -->|"JSON"| K["üì¶ Kafka"]
    
    style PG fill:#336791,color:#fff
    style DEB fill:#e8f5e9
```

#### Configuration PostgreSQL requise

```sql
-- Activer la r√©plication logique (postgresql.conf)
-- wal_level = logical
-- max_replication_slots = 4
-- max_wal_senders = 4

-- Cr√©er un slot de r√©plication
SELECT * FROM pg_create_logical_replication_slot('debezium', 'pgoutput');

-- V√©rifier les slots
SELECT slot_name, plugin, slot_type, active FROM pg_replication_slots;
```

#### Configuration Debezium PostgreSQL

```json
{
  "name": "postgres-cdc-source",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "postgres",
    "database.password": "postgres",
    "database.dbname": "orders_db",
    "database.server.name": "orders",
    "plugin.name": "pgoutput",
    "slot.name": "debezium",
    "publication.name": "dbz_publication",
    "table.include.list": "public.orders,public.customers",
    "topic.prefix": "cdc",
    "schema.history.internal.kafka.bootstrap.servers": "kafka:29092",
    "schema.history.internal.kafka.topic": "schema-changes.orders"
  }
}
```

---

### 7. CDC avec SQL Server

SQL Server utilise le **Change Tracking** ou **CDC natif** pour capturer les modifications.

```mermaid
flowchart TB
    subgraph SQLServer["üî∑ SQL Server"]
        APP["üñ•Ô∏è Application .NET"] -->|"Entity Framework"| SQL["SQL Server"]
        SQL -->|"CDC Tables"| CT["üìù cdc.*_CT"]
    end
    
    CT -->|"Poll changes"| DEB["üîå Debezium"]
    DEB -->|"JSON"| K["üì¶ Kafka"]
    
    style SQL fill:#cc2927,color:#fff
    style DEB fill:#e8f5e9
```

#### Activation CDC sur SQL Server

```sql
-- Activer CDC sur la base de donn√©es
USE orders_db;
EXEC sys.sp_cdc_enable_db;

-- Activer CDC sur une table
EXEC sys.sp_cdc_enable_table
  @source_schema = N'dbo',
  @source_name = N'Orders',
  @role_name = NULL,
  @supports_net_changes = 1;

-- V√©rifier les tables CDC
SELECT name, is_tracked_by_cdc FROM sys.tables WHERE is_tracked_by_cdc = 1;

-- V√©rifier le statut CDC
EXEC sys.sp_cdc_help_change_data_capture;
```

#### Configuration Debezium SQL Server

```json
{
  "name": "sqlserver-cdc-source",
  "config": {
    "connector.class": "io.debezium.connector.sqlserver.SqlServerConnector",
    "database.hostname": "sqlserver",
    "database.port": "1433",
    "database.user": "sa",
    "database.password": "YourStrong!Passw0rd",
    "database.names": "orders_db",
    "topic.prefix": "sqlserver",
    "table.include.list": "dbo.Orders,dbo.Customers",
    "database.encrypt": "false",
    "schema.history.internal.kafka.bootstrap.servers": "kafka:29092",
    "schema.history.internal.kafka.topic": "schema-changes.sqlserver"
  }
}
```

---

### 8. Comparaison PostgreSQL vs SQL Server CDC

| Crit√®re | PostgreSQL | SQL Server |
|---------|------------|------------|
| **M√©canisme** | Logical Replication (WAL) | CDC Tables (polling) |
| **Latence** | ~100ms (temps r√©el) | ~1-5s (polling interval) |
| **Impact performance** | Faible | Mod√©r√© |
| **Configuration** | `wal_level=logical` | `sp_cdc_enable_db` |
| **DELETE** | Captur√© | Captur√© |
| **Schema changes** | Automatique | Reconfiguration requise |

#### Bonnes pratiques CDC

> **‚ö†Ô∏è Production** : Toujours tester le CDC en staging avant production

```text
‚úÖ DO:
  - Monitorer le lag des slots de r√©plication
  - Configurer la r√©tention des slots
  - Utiliser des topics s√©par√©s par table
  - Activer la compression des topics CDC

‚ùå DON'T:
  - Activer CDC sur toutes les tables
  - Ignorer le monitoring des slots
  - Oublier de nettoyer les anciens slots
```

---

## üîå Ports et Services

| Service | Port | Description |
|---------|------|-------------|
| Kafka Connect | 8083 | REST API |
| Kafka UI | 8080 | Interface web |
| Kafka | 9092 | Broker |

---

## üõ†Ô∏è Partie Pratique (70%)

### Pr√©requis

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
cd formation-v2/
./scripts/up.sh
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# V√©rifier que le cluster Kafka est pr√™t
kubectl get kafka -n kafka
kubectl get pods -n kafka -l strimzi.io/cluster=bhf-kafka

# Kafka Connect avec Strimzi utilise KafkaConnect CR
kubectl get kafkaconnect -n kafka
```

</details>

---

### √âtape 1 - D√©marrer Kafka Connect

```bash
docker compose -f day-03-integration/module-06-kafka-connect/docker-compose.module.yml up -d
```

**V√©rification** :

```bash
# Attendre le d√©marrage (30-60 secondes)
sleep 30

# V√©rifier le statut
curl -s http://localhost:8083/ | jq
```

**R√©sultat attendu** :

```json
{
  "version": "3.6.0",
  "commit": "...",
  "kafka_cluster_id": "..."
}
```

---

### √âtape 2 - Lab 1 : Lister les plugins disponibles

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -s http://localhost:8083/connector-plugins | jq '.[].class'
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Via le Service NodePort Kafka Connect (31083)
curl -s http://localhost:31083/connector-plugins | jq '.[].class'
```

</details>

**R√©sultat attendu** : Liste des connecteurs disponibles (FileStreamSource, FileStreamSink, etc.)

---

### √âtape 3 - Lab 2 : Cr√©er un Source Connector

**Objectif** : Lire un fichier et envoyer son contenu vers Kafka.

#### 3.1 Cr√©er le fichier source

```bash
docker exec kafka-connect sh -c 'echo "Hello Kafka Connect" > /tmp/source-data.txt'
docker exec kafka-connect sh -c 'echo "Line 2" >> /tmp/source-data.txt'
docker exec kafka-connect sh -c 'echo "Line 3" >> /tmp/source-data.txt'
```

#### 3.2 Cr√©er le connecteur

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "file-source",
    "config": {
      "connector.class": "FileStreamSource",
      "tasks.max": "1",
      "file": "/tmp/source-data.txt",
      "topic": "file-topic"
    }
  }'
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
curl -X POST http://localhost:31083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "file-source",
    "config": {
      "connector.class": "FileStreamSource",
      "tasks.max": "1",
      "file": "/tmp/source-data.txt",
      "topic": "file-topic"
    }
  }'
```

</details>

#### 3.3 V√©rifier le statut

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -s http://localhost:8083/connectors/file-source/status | jq
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
curl -s http://localhost:31083/connectors/file-source/status | jq
```

</details>

**R√©sultat attendu** :

```json
{
  "name": "file-source",
  "connector": { "state": "RUNNING" },
  "tasks": [{ "id": 0, "state": "RUNNING" }]
}
```

#### 3.4 V√©rifier les messages dans Kafka

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
docker exec kafka kafka-console-consumer \
  --topic file-topic \
  --from-beginning \
  --max-messages 3 \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
kubectl run kafka-consumer --rm -it --restart=Never \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  -n kafka -- bin/kafka-console-consumer.sh \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic file-topic --from-beginning --max-messages 3
```

</details>

---

### √âtape 4 - Lab 3 : Cr√©er un Sink Connector

**Objectif** : √âcrire les messages Kafka vers un fichier.

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "file-sink",
    "config": {
      "connector.class": "FileStreamSink",
      "tasks.max": "1",
      "file": "/tmp/sink-output.txt",
      "topics": "file-topic"
    }
  }'
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
curl -X POST http://localhost:31083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "file-sink",
    "config": {
      "connector.class": "FileStreamSink",
      "tasks.max": "1",
      "file": "/tmp/sink-output.txt",
      "topics": "file-topic"
    }
  }'
```

</details>

**V√©rifier le fichier de sortie** :

```bash
docker exec kafka-connect cat /tmp/sink-output.txt
```

---

### √âtape 5 - Lab 4 : Ajouter des donn√©es en temps r√©el

```bash
# Ajouter des lignes au fichier source
docker exec kafka-connect sh -c 'echo "New line 4" >> /tmp/source-data.txt'
docker exec kafka-connect sh -c 'echo "New line 5" >> /tmp/source-data.txt'

# V√©rifier la propagation
sleep 5
docker exec kafka-connect cat /tmp/sink-output.txt
```

---

### √âtape 6 - Lab 5 : Gestion des connecteurs

#### 6.1 Lister tous les connecteurs

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -s http://localhost:8083/connectors | jq
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
curl -s http://localhost:31083/connectors | jq
```

</details>

#### 6.2 Obtenir la configuration

```bash
curl -s http://localhost:8083/connectors/file-source/config | jq
```

#### 6.3 Mettre en pause

```bash
curl -X PUT http://localhost:8083/connectors/file-source/pause
curl -s http://localhost:8083/connectors/file-source/status | jq '.connector.state'
```

#### 6.4 Reprendre

```bash
curl -X PUT http://localhost:8083/connectors/file-source/resume
```

#### 6.5 Supprimer

```bash
curl -X DELETE http://localhost:8083/connectors/file-source
```

---

## ‚úÖ Checkpoint de validation

- [ ] Kafka Connect d√©marr√© et accessible sur :8083
- [ ] Source connector cr√©√© et RUNNING
- [ ] Messages visibles dans le topic file-topic
- [ ] Sink connector cr√©√© et √©crit dans le fichier
- [ ] Donn√©es en temps r√©el propag√©es
- [ ] Connecteurs g√©rables via REST API

---

## üîß Troubleshooting

### Connecteur en √©tat FAILED

```bash
# Voir les erreurs
curl -s http://localhost:8083/connectors/file-source/status | jq '.tasks[0].trace'

# Red√©marrer la task
curl -X POST http://localhost:8083/connectors/file-source/tasks/0/restart
```

### Kafka Connect ne d√©marre pas

```bash
docker logs kafka-connect --tail 100 | grep -i error
```

---

## üßπ Nettoyage

```bash
# Supprimer les connecteurs
curl -X DELETE http://localhost:8083/connectors/file-source
curl -X DELETE http://localhost:8083/connectors/file-sink

# Arr√™ter le module
docker compose -f day-03-integration/module-06-kafka-connect/docker-compose.module.yml down
```

---

## üìñ Pour aller plus loin

### Exercices suppl√©mentaires

1. **Cr√©ez un connecteur JDBC** pour importer depuis une base de donn√©es
2. **Configurez un SMT** (Single Message Transform) pour modifier les messages
3. **Testez le mode distribu√©** avec plusieurs workers

### Ressources

- [Kafka Connect Documentation](https://kafka.apache.org/documentation/#connect)
- [Confluent Hub](https://www.confluent.io/hub/) - Marketplace de connecteurs
- [Debezium](https://debezium.io/) - CDC pour Kafka
