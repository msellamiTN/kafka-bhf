# üåä Module 05 - Kafka Streams : Traitement en Temps R√©el

| Dur√©e | Niveau | Pr√©requis |
|-------|--------|-----------|
| 3 heures | Interm√©diaire | Modules 01-04 compl√©t√©s |

## üéØ Objectifs d'apprentissage

√Ä la fin de ce module, vous serez capable de :

- ‚úÖ Comprendre la diff√©rence entre KStream et KTable
- ‚úÖ Cr√©er une application Kafka Streams
- ‚úÖ Impl√©menter des transformations (map, filter, flatMap)
- ‚úÖ R√©aliser des agr√©gations en temps r√©el
- ‚úÖ Effectuer des jointures entre streams et tables

---

## üìö Partie Th√©orique (30%)

### 1. Introduction √† Kafka Streams

#### Qu'est-ce que Kafka Streams ?

**Kafka Streams** est une biblioth√®que Java pour construire des applications de traitement de flux en temps r√©el. Contrairement √† Spark ou Flink, elle ne n√©cessite pas de cluster s√©par√©.

```mermaid
flowchart LR
    subgraph spark["üî• SPARK/FLINK"]
        S1["‚Ä¢ Cluster d√©di√©"]
        S2["‚Ä¢ Ressources ++"]
        S3["‚Ä¢ Batch + Stream"]
    end
    
    subgraph streams["üåä KAFKA STREAMS"]
        K1["‚Ä¢ Simple JAR"]
        K2["‚Ä¢ L√©ger & embarqu√©"]
        K3["‚Ä¢ Stream only"]
    end
    
    style streams fill:#e8f5e9
```

**Cas d'usage Kafka Streams** :
- Enrichissement de donn√©es en temps r√©el
- Agr√©gations continues (compteurs, moyennes)
- D√©tection de patterns / anomalies
- Transformation ETL l√©g√®re

#### Architecture d'une application Kafka Streams

```mermaid
flowchart LR
    IT["üì• INPUT TOPIC"] --> APP
    
    subgraph APP["üåä KAFKA STREAMS APP"]
        direction TB
        subgraph TOP["Topology"]
            SRC["Source"] --> PROC["Processor"] --> SINK["Sink"]
        end
        SS[("üíæ State Store")]
    end
    
    APP --> OT["üì§ OUTPUT TOPIC"]
    
    style APP fill:#e3f2fd
```

---

### 2. KStream vs KTable

#### Concepts fondamentaux

| Concept | KStream | KTable |
|---------|---------|--------|
| **Repr√©sentation** | Flux d'√©v√©nements | Table de donn√©es |
| **S√©mantique** | Append-only (insert) | Update/Delete |
| **Analogie SQL** | INSERT | INSERT + UPDATE |
| **Cas d'usage** | Logs, √©v√©nements | √âtats, lookups |

```mermaid
flowchart LR
    subgraph kstream["üìú KSTREAM (flux)"]
        E1["alice: +10"]
        E2["bob: +5"]
        E3["alice: +20"]
        E4["alice: -5"]
    end
    
    kstream -->|"agr√©gation"| ktable
    
    subgraph ktable["üìä KTABLE (√©tat)"]
        T1["alice: 25"]
        T2["bob: 5"]
    end
    
    style kstream fill:#fff3cd
    style ktable fill:#e8f5e9
```

> **KStream** = Chaque message est un √©v√©nement distinct  
> **KTable** = Derni√®re valeur par cl√© (√©tat courant)

#### Quand utiliser quoi ?

```java
// KStream - pour traiter chaque √©v√©nement individuellement
KStream<String, Order> orders = builder.stream("orders");
orders.filter((key, order) -> order.getAmount() > 100)
      .to("large-orders");

// KTable - pour maintenir un √©tat par cl√©
KTable<String, Customer> customers = builder.table("customers");
// Repr√©sente l'√©tat courant de chaque client
```

---

### 3. Op√©rations de transformation

#### Op√©rations sans √©tat (Stateless)

```mermaid
flowchart LR
    subgraph map["üîÑ MAP (1:1)"]
        MA[A] --> Ma[a]
        MB[B] --> Mb[b]
        MC[C] --> Mc[c]
    end
    
    subgraph filter["üîç FILTER"]
        F1[1] -.->|"‚ùå"| FX[ ]
        F2[2] -->|"‚úÖ"| F2o[2]
        F3[3] -->|"‚úÖ"| F3o[3]
    end
    
    subgraph flatmap["üì§ FLATMAP (1:N)"]
        FM["A B C"] --> FMA[A]
        FM --> FMB[B]
        FM --> FMC[C]
    end
```

```java
// Exemples de code
stream.map((key, value) -> KeyValue.pair(key.toUpperCase(), value * 2))
      .filter((key, value) -> value > 100)
      .flatMapValues(value -> Arrays.asList(value.split(" ")));
```

#### Op√©rations avec √©tat (Stateful)

```mermaid
flowchart LR
    subgraph agg["üìä AGGREGATE"]
        IN1["alice:10, bob:5, alice:15"] --> OUT1["alice:25, bob:5"]
    end
    
    subgraph join["üîó JOIN"]
        ORD["Orders (stream)"] --> J{"‚ãà"}
        CUST["Customers (table)"] --> J
        J --> ENR["Enriched Orders"]
    end
    
    subgraph window["‚è±Ô∏è WINDOWED"]
        W1["[0-5]: 3"]
        W2["[5-10]: 5"]
        W3["[10-15]: 2"]
    end
    
    style agg fill:#e8f5e9
    style join fill:#e3f2fd
    style window fill:#fff3cd
```

---

### 4. Fen√™tres temporelles (Windowing)

```mermaid
gantt
    title Types de fen√™tres temporelles
    dateFormat X
    axisFormat %s
    
    section Tumbling
    Window 1 :0, 5
    Window 2 :5, 10
    Window 3 :10, 15
    
    section Hopping
    Window A :0, 10
    Window B :5, 15
    Window C :10, 20
    
    section Session
    Session 1 :0, 3
    Session 2 :7, 12
    Session 3 :18, 20
```

| Type | Description |
|------|-------------|
| **Tumbling** | Fen√™tres fixes, pas de chevauchement |
| **Hopping** | Fen√™tres glissantes, chevauchement possible |
| **Session** | Bas√© sur l'inactivit√© (gap) |

```java
// Tumbling window de 5 minutes
stream.groupByKey()
      .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))
      .count();

// Hopping window: 10 min size, 5 min advance
stream.groupByKey()
      .windowedBy(TimeWindows.ofSizeAndGrace(
          Duration.ofMinutes(10), 
          Duration.ofMinutes(1))
          .advanceBy(Duration.ofMinutes(5)))
      .count();

// Session window avec 30 min d'inactivit√©
stream.groupByKey()
      .windowedBy(SessionWindows.ofInactivityGapWithNoGrace(Duration.ofMinutes(30)))
      .count();
```

---

## üîå Ports et Services

| Service | Port | Description |
|---------|------|-------------|
| Kafka Streams App | 18084 | Application de traitement |
| Kafka UI | 8080 | Visualisation des topics |
| Kafka | 9092 | Broker externe |

---

## üõ†Ô∏è Partie Pratique (70%)

### Pr√©requis

```bash
cd formation-v2/
./scripts/up.sh
```

---

### √âtape 1 - Cr√©er les topics

```bash
# Topic d'entr√©e - √©v√©nements de vente
docker exec kafka kafka-topics --create \
  --topic sales-events \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Topic de sortie - ventes par produit
docker exec kafka kafka-topics --create \
  --topic sales-by-product \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Topic de sortie - ventes par fen√™tre temporelle
docker exec kafka kafka-topics --create \
  --topic sales-per-minute \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Table des produits (r√©f√©rentiel)
docker exec kafka kafka-topics --create \
  --topic products \
  --partitions 3 \
  --replication-factor 1 \
  --config cleanup.policy=compact \
  --bootstrap-server localhost:9092
```

---

### √âtape 2 - D√©marrer l'application Kafka Streams

```bash
docker compose -f day-02-development/module-05-kafka-streams/docker-compose.module.yml up -d --build
```

**V√©rification** :

```bash
docker logs m05-streams-app --tail 20
```

---

### √âtape 3 - Lab 1 : Transformation simple (map/filter)

**Objectif** : Filtrer les ventes > 100‚Ç¨ et transformer le format.

#### 3.1 Charger les donn√©es de r√©f√©rence (produits)

```bash
# Ajouter des produits dans la KTable
echo 'PROD-001:{"id":"PROD-001","name":"Laptop","category":"Electronics"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092

echo 'PROD-002:{"id":"PROD-002","name":"Phone","category":"Electronics"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092

echo 'PROD-003:{"id":"PROD-003","name":"Book","category":"Books"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092
```

#### 3.2 Envoyer des √©v√©nements de vente

```bash
# Via l'API
curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-001", "quantity": 2, "unitPrice": 999.99}'

curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-002", "quantity": 1, "unitPrice": 50.00}'

curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-003", "quantity": 5, "unitPrice": 25.00}'
```

#### 3.3 V√©rifier les r√©sultats

```bash
# Ventes filtr√©es (> 100‚Ç¨)
docker exec kafka kafka-console-consumer \
  --topic large-sales \
  --from-beginning \
  --max-messages 5 \
  --bootstrap-server localhost:9092
```

---

### √âtape 4 - Lab 2 : Agr√©gation par produit

**Objectif** : Compter les ventes totales par produit.

```bash
# Observer les agr√©gations
curl -s http://localhost:18084/api/v1/stats/by-product | jq
```

**R√©sultat attendu** :

```json
{
  "PROD-001": { "count": 2, "totalAmount": 1999.98 },
  "PROD-002": { "count": 1, "totalAmount": 50.00 },
  "PROD-003": { "count": 5, "totalAmount": 125.00 }
}
```

---

### √âtape 5 - Lab 3 : Fen√™tres temporelles

**Objectif** : Agr√©ger les ventes par minute.

#### 5.1 G√©n√©rer un flux continu de ventes

```bash
# Script de g√©n√©ration (30 secondes)
for i in {1..10}; do
  curl -s -X POST "http://localhost:18084/api/v1/sales" \
    -H "Content-Type: application/json" \
    -d "{\"productId\": \"PROD-00$((RANDOM % 3 + 1))\", \"quantity\": $((RANDOM % 5 + 1)), \"unitPrice\": $((RANDOM % 100 + 10))}"
  sleep 3
done
```

#### 5.2 Observer les agr√©gations par fen√™tre

```bash
curl -s http://localhost:18084/api/v1/stats/per-minute | jq
```

---

### √âtape 6 - Lab 4 : Jointure Stream-Table

**Objectif** : Enrichir les ventes avec les informations produit.

```bash
# Consommer le topic enrichi
docker exec kafka kafka-console-consumer \
  --topic enriched-sales \
  --from-beginning \
  --max-messages 5 \
  --bootstrap-server localhost:9092
```

**R√©sultat attendu** : Chaque vente contient maintenant le nom et la cat√©gorie du produit.

---

### √âtape 7 - Lab 5 : Interactive Queries

**Objectif** : Requ√™ter l'√©tat local de Kafka Streams.

```bash
# √âtat du store local
curl -s http://localhost:18084/api/v1/stores/sales-by-product/all | jq

# Requ√™te par cl√©
curl -s http://localhost:18084/api/v1/stores/sales-by-product/PROD-001 | jq
```

---

## ‚úÖ Checkpoint de validation

- [ ] Topics cr√©√©s (sales-events, sales-by-product, etc.)
- [ ] Application Kafka Streams d√©marr√©e
- [ ] Transformation map/filter fonctionnelle
- [ ] Agr√©gation par produit observable
- [ ] Fen√™tres temporelles configur√©es
- [ ] Jointure stream-table test√©e
- [ ] Interactive queries fonctionnelles

---

## üîß Troubleshooting

### Application ne d√©marre pas

```bash
docker logs m05-streams-app --tail 100 | grep -i error
```

### State store non disponible

```bash
# V√©rifier l'√©tat de l'application
curl -s http://localhost:18084/api/v1/health
```

### Donn√©es non agr√©g√©es

**Cause possible** : Pas assez de messages ou mauvais partitionnement.

```bash
# V√©rifier le nombre de messages
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic sales-events
```

---

## üßπ Nettoyage

```bash
docker compose -f day-02-development/module-05-kafka-streams/docker-compose.module.yml down

# Supprimer les topics
docker exec kafka kafka-topics --delete --topic sales-events --bootstrap-server localhost:9092
docker exec kafka kafka-topics --delete --topic sales-by-product --bootstrap-server localhost:9092
docker exec kafka kafka-topics --delete --topic products --bootstrap-server localhost:9092
```

---

## üìñ Pour aller plus loin

### Exercices suppl√©mentaires

1. **Ajoutez une fen√™tre glissante** de 10 minutes avec avance de 1 minute
2. **Impl√©mentez une alerte** quand les ventes d√©passent un seuil
3. **Cr√©ez une jointure KStream-KStream** avec une fen√™tre de temps

### Ressources

- [Kafka Streams Documentation](https://kafka.apache.org/documentation/streams/)
- [Confluent Kafka Streams Tutorial](https://developer.confluent.io/tutorials/)
- [Kafka Streams Interactive Queries](https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html)
