# üîÑ Guide de Migration : MQ Traditionnel ‚Üí Apache Kafka

> **Pour √©quipes .NET** migrant d'une architecture monolithique vers microservices

## üìã Contexte

Ce guide s'adresse aux d√©veloppeurs .NET Core exp√©riment√©s qui :
- Utilisent actuellement un syst√®me de messaging traditionnel (IBM MQ, RabbitMQ, MSMQ)
- Migrent d'une architecture monolithique vers des microservices
- N'ont pas de connaissance pr√©alable de Kafka

---

## üéØ Pourquoi migrer vers Kafka ?

### Comparaison MQ Traditionnel vs Kafka

```mermaid
flowchart LR
    subgraph MQ["üì¨ MQ Traditionnel"]
        P1["Producer"] -->|"push"| Q["Queue"]
        Q -->|"consume & delete"| C1["Consumer"]
    end
    
    subgraph Kafka["üì¶ Apache Kafka"]
        P2["Producer"] -->|"append"| T["Topic/Log"]
        T -->|"read"| C2["Consumer A"]
        T -->|"read"| C3["Consumer B"]
    end
    
    style Q fill:#ffcccc
    style T fill:#ccffcc
```

| Crit√®re | MQ Traditionnel | Apache Kafka |
|---------|-----------------|--------------|
| **Mod√®le** | Queue (message supprim√© apr√®s lecture) | Log (message persist√©) |
| **Consumers** | Un seul consumer par message | Multiple consumers (replay possible) |
| **Throughput** | ~10K msg/sec | ~1M+ msg/sec |
| **R√©tention** | Jusqu'√† consommation | Configurable (jours/semaines) |
| **Ordering** | Par queue | Par partition |
| **Scalabilit√©** | Verticale | Horizontale |

### Cas d'usage favorisant Kafka

```mermaid
flowchart TB
    subgraph UseCases["‚úÖ Kafka est id√©al pour"]
        UC1["üìä Event Sourcing"]
        UC2["üîÑ CDC (Change Data Capture)"]
        UC3["üìà Analytics temps r√©el"]
        UC4["üîó Int√©gration microservices"]
        UC5["üìù Audit trail"]
    end
```

---

## üèóÔ∏è Architecture : Monolithique ‚Üí Event-Driven

### Avant : Architecture Monolithique

```mermaid
flowchart TB
    subgraph Monolith["üè¢ Monolithe .NET"]
        UI["üñ•Ô∏è UI Layer"]
        BL["‚öôÔ∏è Business Logic"]
        DAL["üóÑÔ∏è Data Access Layer"]
    end
    
    DAL --> DB[("üêò PostgreSQL")]
    
    style Monolith fill:#ffcccc
```

### Apr√®s : Architecture Event-Driven avec Kafka

```mermaid
flowchart TB
    subgraph Services["‚òÅÔ∏è Microservices .NET"]
        S1["üõí Order Service"]
        S2["üí≥ Payment Service"]
        S3["üì¶ Shipping Service"]
        S4["üìß Notification Service"]
    end
    
    S1 -->|"OrderCreated"| K[("üì¶ Kafka")]
    K -->|"subscribe"| S2
    K -->|"subscribe"| S3
    K -->|"subscribe"| S4
    
    S1 --> DB1[("DB1")]
    S2 --> DB2[("DB2")]
    S3 --> DB3[("DB3")]
    
    style K fill:#ccffcc
```

---

## üîÄ Patterns de Migration

### Pattern 1 : Strangler Fig (√âtranglement progressif)

Migrer progressivement en rempla√ßant les composants un par un.

```mermaid
flowchart LR
    subgraph Phase1["Phase 1"]
        M1["Monolithe"] --> MQ1["MQ"]
    end
    
    subgraph Phase2["Phase 2"]
        M2["Monolithe"] --> K2["Kafka"]
        K2 --> MS2["Nouveau ¬µService"]
    end
    
    subgraph Phase3["Phase 3"]
        MS3a["¬µService A"] --> K3["Kafka"]
        MS3b["¬µService B"] --> K3
    end
    
    Phase1 --> Phase2 --> Phase3
```

### Pattern 2 : Event Sourcing

Stocker l'√©tat comme une s√©quence d'√©v√©nements.

```mermaid
flowchart LR
    subgraph Events["üìù Event Store (Kafka)"]
        E1["OrderCreated"]
        E2["PaymentReceived"]
        E3["OrderShipped"]
    end
    
    E1 --> E2 --> E3
    
    E3 -->|"replay"| S["üìä Current State"]
    
    style Events fill:#e8f5e9
```

```csharp
// √âv√©nement de domaine
public record OrderCreated(
    Guid OrderId,
    Guid CustomerId,
    List<OrderItem> Items,
    decimal TotalAmount,
    DateTime CreatedAt
);

// Publication vers Kafka
await _producer.ProduceAsync("orders.events", new Message<string, string>
{
    Key = order.Id.ToString(),
    Value = JsonSerializer.Serialize(new OrderCreated(...))
});
```

### Pattern 3 : CQRS avec Kafka

S√©parer les commandes (write) des requ√™tes (read).

```mermaid
flowchart TB
    subgraph Write["‚úèÔ∏è Write Side"]
        CMD["Command"] --> WS["Write Service"]
        WS --> DB1[("Write DB")]
        WS -->|"Event"| K["üì¶ Kafka"]
    end
    
    subgraph Read["üìñ Read Side"]
        K -->|"Consume"| RS["Read Service"]
        RS --> DB2[("Read DB")]
        Q["Query"] --> RS
    end
    
    style K fill:#fff3cd
```

---

## üìù Mapping des concepts MQ ‚Üí Kafka

| Concept MQ | √âquivalent Kafka | Notes |
|------------|------------------|-------|
| **Queue** | Topic | Un topic peut avoir plusieurs partitions |
| **Message** | Record | Cl√© + Valeur + Headers + Timestamp |
| **Consumer** | Consumer Group | Plusieurs consumers partagent le travail |
| **Acknowledge** | Commit Offset | Manuel ou automatique |
| **Dead Letter Queue** | DLT (Dead Letter Topic) | Topic s√©par√© pour les erreurs |
| **Message Priority** | ‚ùå Non support√© | Utiliser des topics s√©par√©s |
| **Message Expiry** | Retention Policy | Au niveau du topic |

---

## üíª Migration du code .NET

### Avant : IBM MQ / RabbitMQ

```csharp
// RabbitMQ - Producer
using var connection = factory.CreateConnection();
using var channel = connection.CreateModel();

channel.QueueDeclare("orders", durable: true, ...);
channel.BasicPublish("", "orders", null, body);

// RabbitMQ - Consumer
channel.BasicConsume("orders", autoAck: false, consumer);
```

### Apr√®s : Kafka avec Confluent.Kafka

```csharp
// Kafka - Producer
using var producer = new ProducerBuilder<string, string>(config).Build();

await producer.ProduceAsync("orders", new Message<string, string>
{
    Key = orderId,
    Value = JsonSerializer.Serialize(order)
});

// Kafka - Consumer
using var consumer = new ConsumerBuilder<string, string>(config).Build();
consumer.Subscribe("orders");

while (true)
{
    var result = consumer.Consume(cancellationToken);
    ProcessMessage(result.Message.Value);
    consumer.Commit(result);
}
```

### Configuration Kafka recommand√©e (.NET)

```csharp
// Producer Config - Fiabilit√© maximale
var producerConfig = new ProducerConfig
{
    BootstrapServers = "kafka:9092",
    
    // Idempotence pour √©viter les doublons
    EnableIdempotence = true,
    Acks = Acks.All,
    
    // Retry configuration
    MessageSendMaxRetries = 10,
    RetryBackoffMs = 100,
    
    // Performance
    LingerMs = 5,
    BatchSize = 16384
};

// Consumer Config - Lecture fiable
var consumerConfig = new ConsumerConfig
{
    BootstrapServers = "kafka:9092",
    GroupId = "order-processor",
    
    // Commit manuel pour contr√¥le total
    EnableAutoCommit = false,
    AutoOffsetReset = AutoOffsetReset.Earliest,
    
    // Isolation pour transactions
    IsolationLevel = IsolationLevel.ReadCommitted
};
```

---

## üîÑ Gestion des erreurs : DLQ ‚Üí DLT

### Pattern Dead Letter Topic

```mermaid
flowchart LR
    T["üì• orders"] --> C["Consumer"]
    C -->|"Success"| P["‚úÖ Process"]
    C -->|"Failure (3x)"| DLT["üíÄ orders.DLT"]
    
    style DLT fill:#ffcccc
```

```csharp
public class KafkaConsumerWithDLT
{
    private const int MaxRetries = 3;
    
    public async Task ConsumeAsync(CancellationToken ct)
    {
        while (!ct.IsCancellationRequested)
        {
            var result = _consumer.Consume(ct);
            var retryCount = GetRetryCount(result.Message.Headers);
            
            try
            {
                await ProcessMessage(result.Message);
                _consumer.Commit(result);
            }
            catch (Exception ex)
            {
                if (retryCount >= MaxRetries)
                {
                    // Envoyer vers DLT
                    await SendToDLT(result.Message, ex);
                    _consumer.Commit(result);
                }
                else
                {
                    // Republier avec retry count incr√©ment√©
                    await Republish(result.Message, retryCount + 1);
                    _consumer.Commit(result);
                }
            }
        }
    }
}
```

---

## üè≠ Int√©gration Entity Framework + Kafka

### Pattern Outbox pour garantir la coh√©rence

```mermaid
flowchart LR
    subgraph Transaction["üîí Transaction DB"]
        S["Save Entity"]
        O["Save Outbox Event"]
    end
    
    S --> O
    O --> P["üì§ Outbox Publisher"]
    P --> K["üì¶ Kafka"]
    
    style Transaction fill:#e8f5e9
```

```csharp
// Entity
public class Order
{
    public Guid Id { get; set; }
    public string Status { get; set; }
    public decimal Total { get; set; }
}

// Outbox Event
public class OutboxEvent
{
    public Guid Id { get; set; }
    public string EventType { get; set; }
    public string Payload { get; set; }
    public DateTime CreatedAt { get; set; }
    public DateTime? ProcessedAt { get; set; }
}

// Service avec Outbox Pattern
public class OrderService
{
    public async Task CreateOrderAsync(CreateOrderCommand cmd)
    {
        await using var transaction = await _dbContext.Database
            .BeginTransactionAsync();
        
        try
        {
            // 1. Sauvegarder l'entit√©
            var order = new Order { ... };
            _dbContext.Orders.Add(order);
            
            // 2. Sauvegarder l'√©v√©nement dans l'outbox
            var outboxEvent = new OutboxEvent
            {
                EventType = "OrderCreated",
                Payload = JsonSerializer.Serialize(new OrderCreatedEvent(order))
            };
            _dbContext.OutboxEvents.Add(outboxEvent);
            
            await _dbContext.SaveChangesAsync();
            await transaction.CommitAsync();
        }
        catch
        {
            await transaction.RollbackAsync();
            throw;
        }
    }
}

// Background service pour publier les √©v√©nements
public class OutboxPublisher : BackgroundService
{
    protected override async Task ExecuteAsync(CancellationToken ct)
    {
        while (!ct.IsCancellationRequested)
        {
            var events = await _dbContext.OutboxEvents
                .Where(e => e.ProcessedAt == null)
                .OrderBy(e => e.CreatedAt)
                .Take(100)
                .ToListAsync(ct);
            
            foreach (var evt in events)
            {
                await _producer.ProduceAsync("domain-events", 
                    new Message<string, string>
                    {
                        Key = evt.Id.ToString(),
                        Value = evt.Payload
                    });
                
                evt.ProcessedAt = DateTime.UtcNow;
            }
            
            await _dbContext.SaveChangesAsync(ct);
            await Task.Delay(1000, ct);
        }
    }
}
```

---

## üìä Checklist de migration

### Phase 1 : Pr√©paration
- [ ] Inventaire des queues/topics MQ existants
- [ ] Mapping vers topics Kafka
- [ ] D√©finition des sch√©mas d'√©v√©nements
- [ ] Configuration infrastructure Kafka

### Phase 2 : D√©veloppement
- [ ] Migration des producers
- [ ] Migration des consumers
- [ ] Impl√©mentation DLT
- [ ] Tests d'int√©gration

### Phase 3 : D√©ploiement
- [ ] D√©ploiement Kafka sur OpenShift
- [ ] Migration progressive (dual-write)
- [ ] Monitoring et alerting
- [ ] D√©commissionnement MQ

---

## üîó Ressources compl√©mentaires

- [Module 02 - Producer Reliability (.NET)](../day-01-foundations/module-02-producer-reliability/TUTORIAL-DOTNET.md)
- [Module 03 - Consumer Read Committed (.NET)](../day-01-foundations/module-03-consumer-read-committed/TUTORIAL-DOTNET.md)
- [Module 04 - Advanced Patterns (.NET)](../day-02-development/module-04-advanced-patterns/TUTORIAL-DOTNET.md)
- [Module 06 - Kafka Connect CDC](../day-03-integration/module-06-kafka-connect/README.md)
