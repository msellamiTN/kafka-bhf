# ğŸ“… Day 02 - DÃ©veloppement AvancÃ© & Kafka Streams

> **DurÃ©e estimÃ©e** : 5-6 heures | **Niveau** : IntermÃ©diaire â†’ AvancÃ©

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ã€ la fin de cette journÃ©e, vous serez capable de :

| # | Objectif | Module |
|---|----------|--------|
| 1 | ImplÃ©menter le pattern **Dead Letter Topic** (DLT) | M04 |
| 2 | Configurer des **retries avec backoff exponentiel** | M04 |
| 3 | GÃ©rer le **rebalancing** avec CooperativeSticky | M04 |
| 4 | Distinguer les erreurs **transient vs permanent** | M04 |
| 5 | CrÃ©er une topologie **Kafka Streams** complÃ¨te | M05 |
| 6 | Utiliser **KStream** et **KTable** pour le traitement temps rÃ©el | M05 |
| 7 | ImplÃ©menter des **agrÃ©gations windowed** et des **joins** | M05 |
| 8 | Interroger les **State Stores** via Interactive Queries | M05 |

---

## ğŸ“š Concepts fondamentaux

### Dead Letter Topic (DLT)

```mermaid
flowchart LR
    subgraph Normal["âœ… Flux Normal"]
        P["ğŸ“¤ Producer"] --> T["ğŸ“¦ Topic"]
        T --> C["ğŸ“¥ Consumer"]
        C --> DB[("ğŸ’¾ DB")]
    end
    
    subgraph Error["âŒ Flux Erreur"]
        C -->|"erreur"| R{"ğŸ”„ Retry?"}
        R -->|"max atteint"| DLT["ğŸ’€ DLT"]
        R -->|"retry"| T
    end
    
    style DLT fill:#ffcccc
```

### StratÃ©gie de Retry

| Type d'erreur | Exemple | Action | Retry? |
|---------------|---------|--------|--------|
| **Transient** | Network timeout, DB lock | Retry avec backoff | âœ… Oui |
| **Permanent** | Invalid JSON, Business rule | Envoyer au DLT | âŒ Non |
| **Poison Pill** | Message corrompu | Log + Skip | âŒ Non |

### Backoff Exponentiel

```mermaid
gantt
    title StratÃ©gie de Backoff Exponentiel
    dateFormat X
    axisFormat %s
    
    section Retries
    Tentative 1 (100ms)    :0, 1
    Attente               :1, 2
    Tentative 2 (200ms)    :2, 3
    Attente               :3, 5
    Tentative 3 (400ms)    :5, 6
    Attente               :6, 10
    DLT                    :crit, 10, 11
```

### Kafka Streams - Topologie

```mermaid
flowchart LR
    subgraph Source["ğŸ“¥ Source"]
        IN["sales-events"]
    end
    
    subgraph Processing["âš™ï¸ Processing"]
        F["filter()"]
        M["map()"]
        GB["groupBy()"]
        AGG["aggregate()"]
        J["join()"]
    end
    
    subgraph Sink["ğŸ“¤ Sink"]
        OUT1["large-sales"]
        OUT2["sales-by-product"]
        SS[("State Store")]
    end
    
    IN --> F --> M --> GB --> AGG --> OUT2
    AGG --> SS
    IN --> J --> OUT1
```

### KStream vs KTable

| Aspect | KStream | KTable |
|--------|---------|--------|
| **SÃ©mantique** | Flux d'Ã©vÃ©nements | Table de faits |
| **DonnÃ©es** | Append-only | Upsert (clÃ© unique) |
| **Exemple** | Transactions | Solde compte |
| **OpÃ©ration** | `filter`, `map` | `aggregate`, `reduce` |

```mermaid
flowchart TB
    subgraph KStream["ğŸ“Š KStream (Events)"]
        E1["ğŸ›’ Order #1"]
        E2["ğŸ›’ Order #2"]
        E3["ğŸ›’ Order #3"]
    end
    
    subgraph KTable["ğŸ“‹ KTable (State)"]
        S1["User A: 150â‚¬"]
        S2["User B: 200â‚¬"]
    end
    
    E1 --> E2 --> E3
    KStream -->|"aggregate"| KTable
```

---

## ğŸ’¡ Tips & Best Practices

### Dead Letter Topic

> **ğŸ’€ Toujours crÃ©er un DLT pour chaque topic critique**
> ```java
> @Bean
> public DeadLetterPublishingRecoverer recoverer(KafkaTemplate<?, ?> template) {
>     return new DeadLetterPublishingRecoverer(template,
>         (record, ex) -> new TopicPartition(record.topic() + ".DLT", -1));
> }
> ```

> **ğŸ“Š Monitorer le DLT en production**
> - Alerter si messages dans DLT
> - Dashboard Grafana pour consumer lag du DLT
> - Processus de retraitement manuel

### Kafka Streams

> **ğŸ†” Choisir un APPLICATION_ID unique par application**
> ```java
> props.put(StreamsConfig.APPLICATION_ID_CONFIG, "sales-processor-v1");
> ```

> **ğŸ’¾ Configurer le State Store pour la production**
> ```java
> props.put(StreamsConfig.STATE_DIR_CONFIG, "/var/kafka-streams");
> props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
> ```

> **â° GÃ©rer le temps correctement**
> ```java
> // Utiliser event time, pas processing time
> props.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,
>           WallclockTimestampExtractor.class);
> ```

---

## ğŸ—ï¸ Architecture du Lab

```mermaid
flowchart TB
    subgraph Docker["ğŸ³ Docker Network: bhf-kafka-network"]
        subgraph Infra["Infrastructure"]
            K["ğŸ“¦ Kafka<br/>:29092"]
            UI["ğŸ–¥ï¸ Kafka UI<br/>:8080"]
        end
        
        subgraph M04["Module 04 - Patterns"]
            JAVA04["â˜• Java API<br/>:18082"]
            NET04["ğŸ”· .NET Consumer<br/>:18083"]
        end
        
        subgraph M05["Module 05 - Streams"]
            STREAMS["ğŸŒŠ Streams App<br/>:18084"]
            SS[("ğŸ’¾ State Store")]
        end
    end
    
    JAVA04 -->|"orders"| K
    K -->|"orders"| NET04
    K <-->|"sales-events"| STREAMS
    STREAMS --> SS
    UI --> K
```

---

## ğŸ“¦ Modules

| Module | Titre | DurÃ©e | Description |
|--------|-------|-------|-------------|
| [**M04**](./module-04-advanced-patterns/README.md) | Patterns AvancÃ©s | 90-120 min | DLT, Retry, Rebalancing |
| [**M05**](./module-05-kafka-streams/README.md) | Kafka Streams | 90-120 min | KStream, KTable, Aggregations |

---

## ğŸš€ Quick Start

### PrÃ©requis

- âœ… Day 01 complÃ©tÃ©
- âœ… Kafka infrastructure running

### DÃ©marrer les modules

```powershell
# Depuis formation-v2/
cd infra
docker-compose -f docker-compose.single-node.yml up -d

# Module 04
cd ../day-02-development/module-04-advanced-patterns
docker-compose -f docker-compose.module.yml up -d --build

# Module 05
cd ../module-05-kafka-streams
docker-compose -f docker-compose.module.yml up -d --build
```

### Ports

| Service | Port | Description |
|---------|------|-------------|
| M04 Java API | 18082 | Producer avec DLT |
| M04 .NET Consumer | 18083 | Consumer avec rebalancing |
| M05 Streams App | 18084 | Kafka Streams + REST |

---

## âš ï¸ Erreurs courantes

| Erreur | Cause | Solution |
|--------|-------|----------|
| `StreamsException: task timeout` | Processing trop lent | Augmenter `max.poll.interval.ms` |
| `InvalidStateStoreException` | Store non prÃªt | Attendre `KafkaStreams.State.RUNNING` |
| `SerializationException` | Serde incorrect | VÃ©rifier JsonSerde configuration |
| Message dans DLT | Erreur de traitement | Analyser l'exception dans le header |

---

## â¡ï¸ Navigation

â¬…ï¸ **[Day 01 - Fondamentaux](../day-01-foundations/README.md)**

â¡ï¸ **[Day 03 - IntÃ©gration](../day-03-integration/README.md)**
