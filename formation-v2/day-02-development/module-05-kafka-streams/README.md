# üåä Module 05 - Kafka Streams : Traitement en Temps R√©el

| Dur√©e | Niveau | Pr√©requis |
|-------|--------|-----------|
| 3 heures | Interm√©diaire | Modules 01-04 compl√©t√©s |

## üéØ Objectifs d'apprentissage

√Ä la fin de ce module, vous serez capable de :

- ‚úÖ Comprendre la diff√©rence entre KStream et KTable
- ‚úÖ Cr√©er une application Kafka Streams
- ‚úÖ Impl√©menter des transformations (map, filter, flatMap)
- ‚úÖ R√©aliser des agr√©gations en temps r√©el
- ‚úÖ Effectuer des jointures entre streams et tables

---

## üìö Partie Th√©orique (30%)

### 1. Introduction √† Kafka Streams

#### Qu'est-ce que Kafka Streams ?

**Kafka Streams** est une biblioth√®que Java pour construire des applications de traitement de flux en temps r√©el. Contrairement √† Spark ou Flink, elle ne n√©cessite pas de cluster s√©par√©.

```mermaid
flowchart LR
    subgraph spark["üî• Spark/Flink"]
        S1["Cluster"]
        S2["Heavy"]
    end
    
    subgraph streams["üåä Kafka Streams"]
        K1["JAR"]
        K2["Light"]
    end
    
    style streams fill:#e8f5e9
```

**Cas d'usage Kafka Streams** :
- Enrichissement de donn√©es en temps r√©el
- Agr√©gations continues (compteurs, moyennes)
- D√©tection de patterns / anomalies
- Transformation ETL l√©g√®re

#### Architecture d'une application Kafka Streams

```mermaid
flowchart LR
    IT["üì• Input"] --> SRC["Source"] --> PROC["‚öôÔ∏è Process"] --> SINK["Sink"] --> OT["ÔøΩ Output"]
    PROC -.-> SS[("ÔøΩ State")]
    
    style PROC fill:#e3f2fd
```

---

### 2. KStream vs KTable

#### Concepts fondamentaux

| Concept | KStream | KTable |
|---------|---------|--------|
| **Repr√©sentation** | Flux d'√©v√©nements | Table de donn√©es |
| **S√©mantique** | Append-only (insert) | Update/Delete |
| **Analogie SQL** | INSERT | INSERT + UPDATE |
| **Cas d'usage** | Logs, √©v√©nements | √âtats, lookups |

```mermaid
flowchart LR
    subgraph ks["üìú KStream"]
        E1["a:+10, b:+5, a:+20"]
    end
    ks -->|"Œ£"| kt
    subgraph kt["üìä KTable"]
        T1["a:30, b:5"]
    end
    style ks fill:#fff3cd
    style kt fill:#e8f5e9
```

> **KStream** = Chaque message est un √©v√©nement distinct  
> **KTable** = Derni√®re valeur par cl√© (√©tat courant)

#### Quand utiliser quoi ?

```java
// KStream - pour traiter chaque √©v√©nement individuellement
KStream<String, Order> orders = builder.stream("orders");
orders.filter((key, order) -> order.getAmount() > 100)
      .to("large-orders");

// KTable - pour maintenir un √©tat par cl√©
KTable<String, Customer> customers = builder.table("customers");
// Repr√©sente l'√©tat courant de chaque client
```

---

### 3. Op√©rations de transformation

#### Op√©rations sans √©tat (Stateless)

```mermaid
flowchart LR
    subgraph ops["STATELESS OPS"]
        M["map: A‚Üía"]
        F["filter: [1,2,3]‚Üí[2,3]"]
        FM["flatMap: 'AB'‚Üí[A,B]"]
    end
```

```java
// Exemples de code
stream.map((key, value) -> KeyValue.pair(key.toUpperCase(), value * 2))
      .filter((key, value) -> value > 100)
      .flatMapValues(value -> Arrays.asList(value.split(" ")));
```

#### Op√©rations avec √©tat (Stateful)

```mermaid
flowchart LR
    subgraph stateful["STATEFUL OPS"]
        AGG["üìä aggregate"]
        JOIN["üîó join"]
        WIN["‚è±Ô∏è window"]
    end
    
    style AGG fill:#e8f5e9
    style JOIN fill:#e3f2fd
    style WIN fill:#fff3cd
```

---

### 4. Fen√™tres temporelles (Windowing)

```mermaid
gantt
    title Types de fen√™tres temporelles
    dateFormat X
    axisFormat %s
    
    section Tumbling
    Window 1 :0, 5
    Window 2 :5, 10
    Window 3 :10, 15
    
    section Hopping
    Window A :0, 10
    Window B :5, 15
    Window C :10, 20
    
    section Session
    Session 1 :0, 3
    Session 2 :7, 12
    Session 3 :18, 20
```

| Type | Description |
|------|-------------|
| **Tumbling** | Fen√™tres fixes, pas de chevauchement |
| **Hopping** | Fen√™tres glissantes, chevauchement possible |
| **Session** | Bas√© sur l'inactivit√© (gap) |

```java
// Tumbling window de 5 minutes
stream.groupByKey()
      .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))
      .count();

// Hopping window: 10 min size, 5 min advance
stream.groupByKey()
      .windowedBy(TimeWindows.ofSizeAndGrace(
          Duration.ofMinutes(10), 
          Duration.ofMinutes(1))
          .advanceBy(Duration.ofMinutes(5)))
      .count();

// Session window avec 30 min d'inactivit√©
stream.groupByKey()
      .windowedBy(SessionWindows.ofInactivityGapWithNoGrace(Duration.ofMinutes(30)))
      .count();
```

---

## üîå Ports et Services

| Service | Port | Description |
|---------|------|-------------|
| Kafka Streams App | 18084 | Application de traitement |
| Kafka UI | 8080 | Visualisation des topics |
| Kafka | 9092 | Broker externe |

---

## üõ†Ô∏è Partie Pratique (70%)

### Pr√©requis

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
cd formation-v2/
./scripts/up.sh
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# V√©rifier que le cluster Kafka est pr√™t
kubectl get kafka -n kafka
kubectl get pods -n kafka -l strimzi.io/cluster=bhf-kafka
```

</details>

---

### √âtape 1 - Cr√©er les topics

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Topic d'entr√©e - √©v√©nements de vente
docker exec kafka kafka-topics --create \
  --topic sales-events \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Topic de sortie - ventes par produit
docker exec kafka kafka-topics --create \
  --topic sales-by-product \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Topic de sortie - ventes par fen√™tre temporelle
docker exec kafka kafka-topics --create \
  --topic sales-per-minute \
  --partitions 6 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Table des produits (r√©f√©rentiel)
docker exec kafka kafka-topics --create \
  --topic products \
  --partitions 3 \
  --replication-factor 1 \
  --config cleanup.policy=compact \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Cr√©er les topics via KafkaTopic CRs
cat <<EOF | kubectl apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: sales-events
  namespace: kafka
  labels:
    strimzi.io/cluster: bhf-kafka
spec:
  partitions: 6
  replicas: 3
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: sales-by-product
  namespace: kafka
  labels:
    strimzi.io/cluster: bhf-kafka
spec:
  partitions: 6
  replicas: 3
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: sales-per-minute
  namespace: kafka
  labels:
    strimzi.io/cluster: bhf-kafka
spec:
  partitions: 6
  replicas: 3
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: products
  namespace: kafka
  labels:
    strimzi.io/cluster: bhf-kafka
spec:
  partitions: 3
  replicas: 3
  config:
    cleanup.policy: compact
EOF
```

**V√©rification** :

```bash
kubectl get kafkatopics -n kafka | grep -E "sales|products"
```

</details>

---

### √âtape 2 - D√©marrer l'application Kafka Streams

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
docker compose -f day-02-development/module-05-kafka-streams/docker-compose.module.yml up -d --build
```

**V√©rification** :

```bash
docker logs m05-streams-app --tail 20
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Builder et pousser l'image
cd formation-v2/day-02-development/module-05-kafka-streams
docker build -t localhost:5000/m05-streams-app:latest -f java/Dockerfile java/
docker push localhost:5000/m05-streams-app:latest

# D√©ployer sur K8s
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: m05-streams-app
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: m05-streams-app
  template:
    metadata:
      labels:
        app: m05-streams-app
    spec:
      containers:
      - name: streams-app
        image: localhost:5000/m05-streams-app:latest
        ports:
        - containerPort: 8080
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "bhf-kafka-kafka-bootstrap.kafka.svc:9092"
---
apiVersion: v1
kind: Service
metadata:
  name: m05-streams-app
  namespace: kafka
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 31084
  selector:
    app: m05-streams-app
EOF
```

**V√©rification** :

```bash
kubectl logs -n kafka -l app=m05-streams-app --tail 20
```

</details>

---

### √âtape 3 - Lab 1 : Transformation simple (map/filter)

**Objectif** : Filtrer les ventes > 100‚Ç¨ et transformer le format.

#### 3.1 Charger les donn√©es de r√©f√©rence (produits)

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Ajouter des produits dans la KTable
echo 'PROD-001:{"id":"PROD-001","name":"Laptop","category":"Electronics"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092

echo 'PROD-002:{"id":"PROD-002","name":"Phone","category":"Electronics"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092

echo 'PROD-003:{"id":"PROD-003","name":"Book","category":"Books"}' | \
  docker exec -i kafka kafka-console-producer \
    --topic products \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --bootstrap-server localhost:9092
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Ajouter des produits via un pod √©ph√©m√®re
kubectl run kafka-producer --rm -it --restart=Never \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  -n kafka -- bin/kafka-console-producer.sh \
  --topic products \
  --property "parse.key=true" \
  --property "key.separator=:" \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092

# Puis entrez les donn√©es:
# PROD-001:{"id":"PROD-001","name":"Laptop","category":"Electronics"}
# PROD-002:{"id":"PROD-002","name":"Phone","category":"Electronics"}
# PROD-003:{"id":"PROD-003","name":"Book","category":"Books"}
```

</details>

#### 3.2 Envoyer des √©v√©nements de vente

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Via l'API
curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-001", "quantity": 2, "unitPrice": 999.99}'

curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-002", "quantity": 1, "unitPrice": 50.00}'

curl -X POST "http://localhost:18084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-003", "quantity": 5, "unitPrice": 25.00}'
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Via l'API (NodePort 31084)
curl -X POST "http://localhost:31084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-001", "quantity": 2, "unitPrice": 999.99}'

curl -X POST "http://localhost:31084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-002", "quantity": 1, "unitPrice": 50.00}'

curl -X POST "http://localhost:31084/api/v1/sales" \
  -H "Content-Type: application/json" \
  -d '{"productId": "PROD-003", "quantity": 5, "unitPrice": 25.00}'
```

</details>

#### 3.3 V√©rifier les r√©sultats

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Ventes filtr√©es (> 100‚Ç¨)
docker exec kafka kafka-console-consumer \
  --topic large-sales \
  --from-beginning \
  --max-messages 5 \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
kubectl run kafka-consumer --rm -it --restart=Never \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  -n kafka -- bin/kafka-console-consumer.sh \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic large-sales --from-beginning --max-messages 5
```

</details>

---

### √âtape 4 - Lab 2 : Agr√©gation par produit

**Objectif** : Compter les ventes totales par produit.

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Observer les agr√©gations
curl -s http://localhost:18084/api/v1/stats/by-product | jq
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Observer les agr√©gations (NodePort 31084)
curl -s http://localhost:31084/api/v1/stats/by-product | jq
```

</details>

**R√©sultat attendu** :

```json
{
  "PROD-001": { "count": 2, "totalAmount": 1999.98 },
  "PROD-002": { "count": 1, "totalAmount": 50.00 },
  "PROD-003": { "count": 5, "totalAmount": 125.00 }
}
```

---

### √âtape 5 - Lab 3 : Fen√™tres temporelles

**Objectif** : Agr√©ger les ventes par minute.

#### 5.1 G√©n√©rer un flux continu de ventes

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Script de g√©n√©ration (30 secondes)
for i in {1..10}; do
  curl -s -X POST "http://localhost:18084/api/v1/sales" \
    -H "Content-Type: application/json" \
    -d "{\"productId\": \"PROD-00$((RANDOM % 3 + 1))\", \"quantity\": $((RANDOM % 5 + 1)), \"unitPrice\": $((RANDOM % 100 + 10))}"
  sleep 3
done
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# Script de g√©n√©ration (NodePort 31084)
for i in {1..10}; do
  curl -s -X POST "http://localhost:31084/api/v1/sales" \
    -H "Content-Type: application/json" \
    -d "{\"productId\": \"PROD-00$((RANDOM % 3 + 1))\", \"quantity\": $((RANDOM % 5 + 1)), \"unitPrice\": $((RANDOM % 100 + 10))}"
  sleep 3
done
```

</details>

#### 5.2 Observer les agr√©gations par fen√™tre

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
curl -s http://localhost:18084/api/v1/stats/per-minute | jq
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
curl -s http://localhost:31084/api/v1/stats/per-minute | jq
```

</details>

---

### √âtape 6 - Lab 4 : Jointure Stream-Table

**Objectif** : Enrichir les ventes avec les informations produit.

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# Consommer le topic enrichi
docker exec kafka kafka-console-consumer \
  --topic enriched-sales \
  --from-beginning \
  --max-messages 5 \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
kubectl run kafka-consumer --rm -it --restart=Never \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  -n kafka -- bin/kafka-console-consumer.sh \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic enriched-sales --from-beginning --max-messages 5
```

</details>

**R√©sultat attendu** : Chaque vente contient maintenant le nom et la cat√©gorie du produit.

---

### √âtape 7 - Lab 5 : Interactive Queries

**Objectif** : Requ√™ter l'√©tat local de Kafka Streams.

<details>
<summary>üê≥ <b>Mode Docker</b></summary>

```bash
# √âtat du store local
curl -s http://localhost:18084/api/v1/stores/sales-by-product/all | jq

# Requ√™te par cl√©
curl -s http://localhost:18084/api/v1/stores/sales-by-product/PROD-001 | jq
```

</details>

<details>
<summary>‚ò∏Ô∏è <b>Mode OKD/K3s</b></summary>

```bash
# √âtat du store local (NodePort 31084)
curl -s http://localhost:31084/api/v1/stores/sales-by-product/all | jq

# Requ√™te par cl√©
curl -s http://localhost:31084/api/v1/stores/sales-by-product/PROD-001 | jq
```

</details>

---

## ‚úÖ Checkpoint de validation

- [ ] Topics cr√©√©s (sales-events, sales-by-product, etc.)
- [ ] Application Kafka Streams d√©marr√©e
- [ ] Transformation map/filter fonctionnelle
- [ ] Agr√©gation par produit observable
- [ ] Fen√™tres temporelles configur√©es
- [ ] Jointure stream-table test√©e
- [ ] Interactive queries fonctionnelles

---

## üîß Troubleshooting

### Application ne d√©marre pas

```bash
docker logs m05-streams-app --tail 100 | grep -i error
```

### State store non disponible

```bash
# V√©rifier l'√©tat de l'application
curl -s http://localhost:18084/api/v1/health
```

### Donn√©es non agr√©g√©es

**Cause possible** : Pas assez de messages ou mauvais partitionnement.

```bash
# V√©rifier le nombre de messages
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic sales-events
```

---

## üßπ Nettoyage

```bash
docker compose -f day-02-development/module-05-kafka-streams/docker-compose.module.yml down

# Supprimer les topics
docker exec kafka kafka-topics --delete --topic sales-events --bootstrap-server localhost:9092
docker exec kafka kafka-topics --delete --topic sales-by-product --bootstrap-server localhost:9092
docker exec kafka kafka-topics --delete --topic products --bootstrap-server localhost:9092
```

---

## üìñ Pour aller plus loin

### Exercices suppl√©mentaires

1. **Ajoutez une fen√™tre glissante** de 10 minutes avec avance de 1 minute
2. **Impl√©mentez une alerte** quand les ventes d√©passent un seuil
3. **Cr√©ez une jointure KStream-KStream** avec une fen√™tre de temps

### Ressources

- [Kafka Streams Documentation](https://kafka.apache.org/documentation/streams/)
- [Confluent Kafka Streams Tutorial](https://developer.confluent.io/tutorials/)
- [Kafka Streams Interactive Queries](https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html)
